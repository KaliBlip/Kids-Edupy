{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35c88926",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Test the trained model on a sample input\u001b[39;00m\n\u001b[32m      2\u001b[39m sample = \u001b[33m\"\u001b[39m\u001b[33mI are going to the store\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m inputs = \u001b[43mtokenizer\u001b[49m(sample, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m outputs = model.generate(**inputs, max_length=\u001b[32m64\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mInput:\u001b[39m\u001b[33m\"\u001b[39m, sample)\n",
      "\u001b[31mNameError\u001b[39m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "# Test the trained model on a sample input\n",
    "sample = \"I are going to the store\"\n",
    "inputs = tokenizer(sample, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_length=64)\n",
    "print(\"Input:\", sample)\n",
    "print(\"Correction:\", tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19fd14ee",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "\nT5Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m train_data \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_text\u001b[39m\u001b[38;5;124m\"\u001b[39m: inputs[:\u001b[38;5;241m100\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtarget_text\u001b[39m\u001b[38;5;124m\"\u001b[39m: targets[:\u001b[38;5;241m100\u001b[39m]}\n\u001b[0;32m      7\u001b[0m dataset \u001b[38;5;241m=\u001b[39m Dataset\u001b[38;5;241m.\u001b[39mfrom_dict(train_data)\n\u001b[1;32m----> 9\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mT5Tokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mt5-small\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpreprocess\u001b[39m(example):\n\u001b[0;32m     12\u001b[0m     input_enc \u001b[38;5;241m=\u001b[39m tokenizer(example[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_text\u001b[39m\u001b[38;5;124m\"\u001b[39m], truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\TecFielD\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\utils\\import_utils.py:2132\u001b[0m, in \u001b[0;36mDummyObject.__getattribute__\u001b[1;34m(cls, key)\u001b[0m\n\u001b[0;32m   2130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (key\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_config\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_dummy\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmro\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcall\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   2131\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(key)\n\u001b[1;32m-> 2132\u001b[0m \u001b[43mrequires_backends\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backends\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\TecFielD\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\utils\\import_utils.py:2118\u001b[0m, in \u001b[0;36mrequires_backends\u001b[1;34m(obj, backends)\u001b[0m\n\u001b[0;32m   2115\u001b[0m         failed\u001b[38;5;241m.\u001b[39mappend(msg\u001b[38;5;241m.\u001b[39mformat(name))\n\u001b[0;32m   2117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m failed:\n\u001b[1;32m-> 2118\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(failed))\n",
      "\u001b[1;31mImportError\u001b[0m: \nT5Tokenizer requires the SentencePiece library but it was not found in your environment. Check out the instructions on the\ninstallation page of its repo: https://github.com/google/sentencepiece#installation and follow the ones\nthat match your environment. Please note that you may need to restart your runtime after installation.\n"
     ]
    }
   ],
   "source": [
    "# Fine-tune T5-small model (demo: just show setup, not full training for speed)\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, Trainer, TrainingArguments\n",
    "from datasets import Dataset\n",
    "\n",
    "# Use a small subset for demo\n",
    "train_data = {\"input_text\": inputs[:100], \"target_text\": targets[:100]}\n",
    "dataset = Dataset.from_dict(train_data)\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "\n",
    "def preprocess(example):\n",
    "    input_enc = tokenizer(example[\"input_text\"], truncation=True, padding=\"max_length\", max_length=64)\n",
    "    target_enc = tokenizer(example[\"target_text\"], truncation=True, padding=\"max_length\", max_length=64)\n",
    "    return {\n",
    "        \"input_ids\": input_enc[\"input_ids\"],\n",
    "        \"attention_mask\": input_enc[\"attention_mask\"],\n",
    "        \"labels\": target_enc[\"input_ids\"]\n",
    "    }\n",
    "\n",
    "dataset = dataset.map(preprocess)\n",
    "\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=1,\n",
    "    output_dir=\"./t5-demo\",\n",
    "    logging_steps=5,\n",
    "    save_steps=20,\n",
    "    report_to=[],\n",
    "    fp16=False\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "print(\"Demo training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f8c8e7bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 25.2 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\TecFielD\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "719e90fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tf-keras\n",
      "  Downloading tf_keras-2.19.0-py3-none-any.whl (1.7 MB)\n",
      "Collecting tensorflow<2.20,>=2.19\n",
      "  Downloading tensorflow-2.19.1-cp310-cp310-win_amd64.whl (375.7 MB)\n",
      "Collecting ml-dtypes<1.0.0,>=0.5.1\n",
      "  Downloading ml_dtypes-0.5.3-cp310-cp310-win_amd64.whl (206 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\tecfield\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (25.0)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting numpy<2.2.0,>=1.26.0\n",
      "  Downloading numpy-2.1.3-cp310-cp310-win_amd64.whl (12.9 MB)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.74.0-cp310-cp310-win_amd64.whl (4.5 MB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\tecfield\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (4.14.1)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.31.0-cp310-cp310-win_amd64.whl (1.5 MB)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Downloading absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
      "Collecting libclang>=13.0.0\n",
      "  Downloading libclang-18.1.1-py2.py3-none-win_amd64.whl (26.4 MB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Downloading termcolor-3.1.0-py3-none-any.whl (7.7 kB)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting wrapt>=1.11.0\n",
      "  Downloading wrapt-1.17.3-cp310-cp310-win_amd64.whl (38 kB)\n",
      "Collecting tensorboard~=2.19.0\n",
      "  Downloading tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
      "Collecting flatbuffers>=24.3.25\n",
      "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\tecfield\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (2.32.4)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3\n",
      "  Downloading protobuf-5.29.5-cp310-abi3-win_amd64.whl (434 kB)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\tecfield\\appdata\\roaming\\python\\python310\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (1.17.0)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1\n",
      "  Downloading gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Collecting h5py>=3.11.0\n",
      "  Downloading h5py-3.14.0-cp310-cp310-win_amd64.whl (2.9 MB)\n",
      "Collecting keras>=3.5.0\n",
      "  Downloading keras-3.11.2-py3-none-any.whl (1.4 MB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\tecfield\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow<2.20,>=2.19->tf-keras) (57.4.0)\n",
      "Collecting wheel<1.0,>=0.23.0\n",
      "  Downloading wheel-0.45.1-py3-none-any.whl (72 kB)\n",
      "Collecting optree\n",
      "  Downloading optree-0.17.0-cp310-cp310-win_amd64.whl (304 kB)\n",
      "Collecting namex\n",
      "  Downloading namex-0.1.0-py3-none-any.whl (5.9 kB)\n",
      "Collecting rich\n",
      "  Downloading rich-14.1.0-py3-none-any.whl (243 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\tecfield\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (2.5.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\tecfield\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (3.10)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\tecfield\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (3.4.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\tecfield\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow<2.20,>=2.19->tf-keras) (2025.8.3)\n",
      "Collecting werkzeug>=1.0.1\n",
      "  Downloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading markdown-3.8.2-py3-none-any.whl (106 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Collecting MarkupSafe>=2.1.1\n",
      "  Downloading MarkupSafe-3.0.2-cp310-cp310-win_amd64.whl (15 kB)\n",
      "Collecting markdown-it-py>=2.2.0\n",
      "  Downloading markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\tecfield\\appdata\\roaming\\python\\python310\\site-packages (from rich->keras>=3.5.0->tensorflow<2.20,>=2.19->tf-keras) (2.19.2)\n",
      "Collecting mdurl~=0.1\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: mdurl, numpy, MarkupSafe, markdown-it-py, wheel, werkzeug, tensorboard-data-server, rich, protobuf, optree, namex, ml-dtypes, markdown, h5py, grpcio, absl-py, wrapt, termcolor, tensorflow-io-gcs-filesystem, tensorboard, opt-einsum, libclang, keras, google-pasta, gast, flatbuffers, astunparse, tensorflow, tf-keras\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.2.6\n",
      "    Uninstalling numpy-2.2.6:\n",
      "      Successfully uninstalled numpy-2.2.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Users\\\\TecFielD\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\Lib\\\\site-packages\\\\~umpy.libs\\\\libscipy_openblas64_-13e2df515630b4a41f92893938845698.dll'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n",
      "WARNING: You are using pip version 21.2.3; however, version 25.2 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\TecFielD\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0c3df6a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers datasets pandas scikit-learn --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02bfee8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample input: So I think we can not live if old people could not find siences and tecnologies and they did not developped. \n",
      "Sample target: So I think we would not be alive if our ancestors did not develop sciences and technologies. \n"
     ]
    }
   ],
   "source": [
    "# Preprocess for seq2seq: create (input, target) pairs\n",
    "inputs = df.iloc[:,0].astype(str).tolist()\n",
    "targets = df.iloc[:,1].astype(str).tolist()\n",
    "print(f\"Sample input: {inputs[0]}\")\n",
    "print(f\"Sample target: {targets[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3936528a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: So I think we can not live if old people could not find siences and tecnologies and they did not developped. \n",
      "Corrected: So I think we would not be alive if our ancestors did not develop sciences and technologies. \n",
      "\n",
      "Original: So I think we can not live if old people could not find siences and tecnologies and they did not developped. \n",
      "Corrected: So I think we could not live if older people did not develop science and technologies. \n",
      "\n",
      "Original: So I think we can not live if old people could not find siences and tecnologies and they did not developped. \n",
      "Corrected: So I think we can not live if old people could not find science and technologies and they did not develop. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show a few original/corrected pairs\n",
    "for i in range(3):\n",
    "    print(f\"Original: {df.iloc[i,0]}\")\n",
    "    print(f\"Corrected: {df.iloc[i,1]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5064929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 3016\n",
      "Columns: Index(['input', 'target'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>So I think we can not live if old people could...</td>\n",
       "      <td>So I think we would not be alive if our ancest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>So I think we can not live if old people could...</td>\n",
       "      <td>So I think we could not live if older people d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>So I think we can not live if old people could...</td>\n",
       "      <td>So I think we can not live if old people could...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>So I think we can not live if old people could...</td>\n",
       "      <td>So I think we can not live if old people can n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>For not use car.</td>\n",
       "      <td>Not for use with a car.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               input  \\\n",
       "0  So I think we can not live if old people could...   \n",
       "1  So I think we can not live if old people could...   \n",
       "2  So I think we can not live if old people could...   \n",
       "3  So I think we can not live if old people could...   \n",
       "4                                  For not use car.    \n",
       "\n",
       "                                              target  \n",
       "0  So I think we would not be alive if our ancest...  \n",
       "1  So I think we could not live if older people d...  \n",
       "2  So I think we can not live if old people could...  \n",
       "3  So I think we can not live if old people can n...  \n",
       "4                           Not for use with a car.   "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and analyze JFLEG train.csv\n",
    "import pandas as pd\n",
    "df = pd.read_csv('jfleg-dataset/versions/1/train.csv')\n",
    "print('Rows:', len(df))\n",
    "print('Columns:', df.columns)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52a77568",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.3; however, version 25.2 is available.\n",
      "You should consider upgrading via the 'C:\\Users\\TecFielD\\AppData\\Local\\Programs\\Python\\Python310\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries\n",
    "!pip install transformers datasets pandas scikit-learn --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2619822",
   "metadata": {},
   "source": [
    "# Grammar Correction Model Development with JFLEG Dataset\n",
    "\n",
    "This notebook demonstrates how to develop a grammar correction model using the JFLEG dataset and Hugging Face Transformers. You'll learn how to:\n",
    "- Install required libraries\n",
    "- Load and analyze the JFLEG data\n",
    "- Preprocess for sequence-to-sequence modeling\n",
    "- Fine-tune a T5-small model\n",
    "- Evaluate and test the model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
